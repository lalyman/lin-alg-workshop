{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baed44d2",
   "metadata": {},
   "source": [
    "<h1>Principal Component Analysis Versus Linear Regression<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94c7ef",
   "metadata": {},
   "source": [
    "* This Jupyter notebook accompanies the **Linear Algebra** course in the [ICME Summer Workshops in Data Science](https://icme.stanford.edu/icme-summer-workshops-2021-fundamentals-data-science#LinAlg) (2021) series. \n",
    "* This course is taught by Laura Lyman and Professor Margot Gerritsen. \n",
    "\n",
    "In this module, we explore some of the differences between ordinary least squares (i.e. linear regression) and principal component analysis. To follow along, you can interatively run and update blocks of code. After clicking into a single code block, type `Shift Enter` to execute it. Keep in mind that some code blocks have dependencies on previous blocks, and therefore the blocks should be evaluated in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e90cba",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ordinary least squares (OLS) and principal component analysis (pca) are both popular techniques in data science for accomplishing similar goals. Both methods pick the \"best\" linear relationships for approximating some response variable $y$ in terms of a set of explanatory variables $x_1, \\ldots, x_p.$ In particular, OLS and pca each pick linear models that minimize the \"error\" between the actual observed values of $y$ and the $y$ values predicted by the model. The key difference is that OLS and pca define \"error\" differently. \n",
    "\n",
    "Before stating symbolically how OLS and pca measure error, we will show such errors visually in the following example. This will provide the geometric intuition that sits at the *heart* of how these methods work. \n",
    "\n",
    "give the visual and geometric intuition at the heart of these measurements. \n",
    "\n",
    "\n",
    "*This module is meant to be a light-hearted, interactive, and hands-on exploration of these data science methods rather than an exhaustive overview of their details.*\n",
    "\n",
    "\n",
    "will show the errors visually in the following example. \n",
    "\n",
    "the intuition at the heart of the  \n",
    "\n",
    "establish the visual and geometric intuition at the heart of \n",
    "\n",
    "intuition at the heart  \n",
    "\n",
    "Instead, we will provide some visual and geometric intuition for what each of these methods is optimizing/minimizing by examining \n",
    "\n",
    "model some dependent (scalar) variable $y$ as a linear function of a set of explanatory variables $x_1, \\ldots, x_p.$ \n",
    "\n",
    "\n",
    "OLS selects the linear function that minimizes the sum of (squared) differences between the observed response values $y_i$ and the values predicted by the linear function of the explanatory variables. This is seen geometrically by the vertical (or horizontal) differences between ...\n",
    "\n",
    "Of course, we say \"the\" because the OLS solution is unique .... and always exists? ...\n",
    "\n",
    "However, pca minimizes the error orthogonal (perpendicular) to the model line\"\n",
    "\n",
    "minimzes the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function of the independent variable.\n",
    "\n",
    "\n",
    "OLS tries to approximate some dependent (scalar) variable $y$ as a linear function of a set of explanatory variables $x_1, \\ldots, x_p.$ \n",
    "\n",
    "In particular, OLS selects such a linear relationship that \"best\" matches the actual observed values of $y$, where the define the best linear model to be the one that minimzes the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function of the independent variable.\n",
    "\n",
    "selects or \"fits\" a linear relationship between a dependent (scalar) variable $y$ and a set of explanatory variables/parameters $x_1, \\ldots, x_p$ that best matches the actual observed data points. \n",
    "\n",
    "In two dimensions, \n",
    "\n",
    "given a dependent/response variable $y$ and some explanatory variables $x_1, \\ldots, x_p$, OLS finds the \"best\" line \n",
    "\n",
    "provide the best linear \n",
    "\n",
    "\n",
    "given explanatory variables $x_1, \\ldots, x_p$ and a variable $y$ that dependeds \n",
    "\n",
    "OLS picks a subset of explanatory variables such that some dependent \n",
    "\n",
    "OLS chooses a subset \n",
    "\n",
    "However, the differences between the two are often glossed over or left unaddressed.\n",
    "\n",
    "1. OLS $y \\sim x$ minimizes vertical distances/residuals\n",
    "2. OLS $x \\sim y$ minimizes horizontal distances/residuals\n",
    "3. pca minimizes the orthogonal projections of the linear approximation onto the actual data\n",
    "\n",
    "The intent of this Jupyter notebook is not to mathematically prove statements 1 - 3, though such arguments can be found in **insert references**. Instead, we will provide some visual and geometric intuition for what each of these methods is optimizing/minimizing by examining \n",
    "\n",
    "In terms of technical details, we assume all variables are real valued. The point of this module is to explore and interact with how these techniques can be applied to real data.. The formal proofs for such things are fine but are reserved for **insert references**...\n",
    "\n",
    "\n",
    "Again, \n",
    "\n",
    "\n",
    "**Abbreviation Glossary**\n",
    "* (pca) principal component analysis as a general technique...\n",
    "* (PCA) Principal Component Analysis (*capitalized*) -- a version of pca (*lowercase*) that specifically utilizes an eigendecomposition/spectral decomposition to compute the principal components and values of a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d332e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   numpy import genfromtxt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Optional; suppresses scientific notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3417ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: Are the columns features, or something else? Is that the right word for them?\n",
    "def center_matrix(X):\n",
    "    [n,p]     = X.shape\n",
    "    X_c       = np.zeros_like(X)\n",
    "    # Subtract off the mean of each property i.e. subtract from each column the avg. of that column \n",
    "    for feature_idx in range(p):\n",
    "        X_c[:,feature_idx] = X[:,feature_idx] - np.mean(X[:,feature_idx])\n",
    "    return(X_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbd260",
   "metadata": {},
   "source": [
    "# Numerical Application & Example: Heart Health\n",
    "\n",
    "Suppose the following matrix $X$ contains resting heart rate (HR) and diastolic blood pressure (BP) data for healthy adolescent women aged 14 - 16. (The data was simulated based off the means and standard deviations from actual patient data given in **academic paper**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[69.57, 76.90], [61.81, 66.66], [71.62, 59.56], \\\n",
    "              [58.59, 69.39], [63.18, 62.82], [62.27, 60.44], \\\n",
    "              [65.96, 72.60], [59.25, 61.56], [63.68, 82.86], \\\n",
    "              [49.48, 42.08]], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots()\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.xlabel(\"Heart Rate (Beats Per Minute)\")\n",
    "plt.ylabel(\"Diastolic Blood Pressure (mm Hg)\")\n",
    "plt.title(\"Simulated Data: Comparing Blood Pressure & \\n Resting Heart Rates for Women\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc730d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X_c = copy.deepcopy(center_matrix(X))\n",
    "X_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe12396",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1/X_c.shape[0] * np.matmul(np.transpose(X_c), X_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54099a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Lambda,V] = np.linalg.eig(C)\n",
    "Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X_c[:,0],X_c[:,1])\n",
    "plt.xlabel(\"Heart Rate (Beats Per Minute)\")\n",
    "plt.ylabel(\"Diastolic Blood Pressure (mm Hg)\")\n",
    "plt.title(\"Simulated Data: Comparing Blood Pressure & \\n Resting Heart Rates for Women\")\n",
    "\n",
    "x_axis = np.arange(np.min(X_c[:,0]), np.max(X_c[:,0]))\n",
    "\n",
    "# Careful! Lambda isn't ordered here like it should be, so actually the second column of V corresponds\n",
    "# to the principal component\n",
    "prin_comp_vec = V[:,1]\n",
    "prin_comp_line = lambda x: prin_comp_vec[1]/prin_comp_vec[0]*x\n",
    "plt.plot(x_axis,prin_comp_line(x_axis))\n",
    "\n",
    "plt.legend(['First Principal Component'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.scatter(X_c[:,0],X_c[:,1])\n",
    "plt.xlabel(\"Heart Rate (Beats Per Minute)\")\n",
    "plt.ylabel(\"Diastolic Blood Pressure (mm Hg)\")\n",
    "plt.title(\"Simulated Data (Centered): Comparing Blood Pressure (BP) & \\n Resting Heart Rates (HR) for Women\")\n",
    "\n",
    "x_axis = np.arange(np.min(X_c[:,0]), np.max(X_c[:,0]))\n",
    "# y_axis = np.arange(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "y_axis = np.arange(-40,30)\n",
    "# Careful! Lambda isn't ordered here like it should be, so actually the second column of V corresponds\n",
    "# to the principal component\n",
    "prin_comp_vec = V[:,1]\n",
    "prin_comp_line = lambda x: prin_comp_vec[1]/prin_comp_vec[0]*x\n",
    "plt.plot(x_axis,prin_comp_line(x_axis))\n",
    "\n",
    "\n",
    "m, b = np.polyfit(X_c[:,0], X_c[:, 1],1)\n",
    "plt.plot(x_axis, m*x_axis + b)\n",
    "\n",
    "m_x, b_x = np.polyfit(X_c[:,1], X_c[:, 0], 1)\n",
    "plt.plot(m_x*y_axis + b_x, y_axis)\n",
    "# pred_DBP = model.predict(x_axis)\n",
    "# print(pred_DBP)\n",
    "# plt.plot(x_axis, pred_DBP)\n",
    "# plt.show()\n",
    "plt.legend(['First Principal Component', 'Linear Regression BP~HR', 'Linear Regression HR~BP'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92da45d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# %matplotlib notebook\n",
    "fig, axs = plt.subplots(1, 3,  figsize=(11, 3), sharey=True)\n",
    "\n",
    "axs[0].set_xlabel(\"Heart Rate (Beats Per Minute)\")\n",
    "axs[1].set_xlabel(\"Heart Rate (Beats Per Minute)\")\n",
    "axs[2].set_xlabel(\"Heart Rate (Beats Per Minute)\")\n",
    "axs[0].set_ylabel(\"Diastolic Blood Pressure (mm Hg)\")\n",
    "\n",
    "axs[0].set_xlim(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "axs[0].set_ylim(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "axs[1].set_xlim(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "axs[1].set_ylim(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "\n",
    "# x_axis = np.arange(np.min(X_c[:,0]), np.max(X_c[:,0]))\n",
    "\n",
    "# Force plots to be square so that we can properly view orthogonal components\n",
    "x_axis = np.arange(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "\n",
    "m, b = np.polyfit(X_c[:,0], X_c[:, 1],1)\n",
    "line, = axs[0].plot(x_axis, m*x_axis + b, color = 'purple')\n",
    "axs[0].vlines(X_c[:,0], m*X_c[:,0] + b, X_c[:,1], linestyles='dotted', color = 'purple', alpha = .4)\n",
    "axs[0].scatter(X_c[:,0],X_c[:,1], c = 'black', alpha = 1)\n",
    "axs[0].legend(['Lin. Regression BP~HR'], loc = 'upper left')\n",
    "\n",
    "# y_axis = np.arange(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "y_axis = np.arange(-40,30)\n",
    "\n",
    "\n",
    "axs[1].plot(m_x*y_axis + b_x, y_axis, color = 'green')\n",
    "axs[1].hlines(X_c[:,1],X_c[:,0], m_x*X_c[:,1] + b_x, linestyles='dotted', color = 'green', alpha = .6)\n",
    "axs[1].legend(['Lin. Regression HR~BP'], loc = 'upper left')\n",
    "axs[1].scatter(X_c[:,0],X_c[:,1], c = 'black', alpha = 1)\n",
    "\n",
    "axs[2].set_xlim(np.min(X_c[:,1]), np.max(X_c[:,1]))\n",
    "\n",
    "axs[2].plot(x_axis,prin_comp_line(x_axis), color = 'red')\n",
    "\n",
    "\n",
    "n = X_c.shape[0]\n",
    "# Iterate through the data to compute orthogonals\n",
    "for i in range(n):\n",
    "    w  = X_c[i,:]\n",
    "    cv = np.dot(prin_comp_vec,w)/np.linalg.norm(prin_comp_vec)*prin_comp_vec\n",
    "    axs[2].plot([w[0],cv[0]],[w[1],cv[1]], linestyle = 'dotted', alpha = .6)\n",
    "axs[2].scatter(X_c[:,0],X_c[:,1], c = 'black', alpha = 1)\n",
    "axs[2].legend(['First Principal Component'], loc = 'upper left')    \n",
    "fig.suptitle(\"Simulated Data (Centered): Comparing Blood Pressure (BP) & Resting Heart Rates (HR) for Women\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38355abd",
   "metadata": {
    "code_folding": [
     19
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.widgets import Slider, Button\n",
    "%matplotlib notebook\n",
    "plt.close('all')\n",
    "\n",
    "# Creates vertical lines between the true data points and the regression line\n",
    "# with slope m and intercept b\n",
    "# Of course assumes x_data_pts and y_data_pts have the same length (since these are coordinate pairs)\n",
    "def make_vlines(x_data_pts, y_data_pts, m, b, first_run = True):\n",
    "    sum_of_vert_dists = np.linalg.norm(m*x_data_pts + b - y_data_pts, 2)\n",
    "    for i in range(x_data_pts.shape[0]):\n",
    "        x_i      = x_data_pts[i]\n",
    "        if first_run:\n",
    "            vline, = ax.plot([x_i, x_i], [y_data_pts[i],  m*x_i + b], linestyle = 'dotted', \\\n",
    "                             alpha = .6, color = 'purple')\n",
    "            the_vlines.append(vline)\n",
    "        else: \n",
    "            the_vlines[i].set_ydata([y_data_pts[i], m*x_i + b])\n",
    "    return(sum_of_vert_dists)\n",
    "            \n",
    "def animate(i):\n",
    "    ax.clear()\n",
    "    line.set_ydata((m+i)*x_axis + b)\n",
    "\n",
    "    vlines = ax.vlines(X_c[:,0], (m+i-1)*X_c[:,0] + b, (m+i)*X_c[:,0] + b, linestyles='solid', color = 'purple')\n",
    "    ax.set_ylim(-25,25)\n",
    "    if i == 9:\n",
    "        del vlines\n",
    "    return line,ax\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# ax = plt.axes()\n",
    "\n",
    "\n",
    "plt.xlabel(\"Heart Rate (Beats Per Minute)\")\n",
    "plt.ylabel(\"Diastolic Blood Pressure (mm Hg)\")\n",
    "\n",
    "plt.scatter(X_c[:,0],X_c[:,1], c = 'purple', alpha = 1)\n",
    "\n",
    "# Adjust the main plot to make room for the sliders\n",
    "plt.subplots_adjust(left=0.25, bottom=0.25)\n",
    "\n",
    "# Make a horizontal slider to control the fitting line\n",
    "slopevals = plt.axes([0.25, 0.1, 0.65, 0.03])\n",
    "slope_slider = Slider(\n",
    "    ax = slopevals,\n",
    "    label='Slope of fitting line',\n",
    "    valmin = -2,\n",
    "    valmax = 2,\n",
    "    valinit = m\n",
    ")\n",
    "\n",
    "# plt.title(\"Simulated Data (Centered): Comparing Blood Pressure & \\n Resting Heart Rates for Women\")\n",
    "\n",
    "x_axis = np.arange(np.min(X_c[:,0]), np.max(X_c[:,0]))\n",
    "x_axis_full = np.arange(-25, 20)\n",
    "# Force plot to look square\n",
    "ax.set_xlim(-25,20)\n",
    "ax.set_ylim(-25,20)\n",
    "\n",
    "\n",
    "m, b       = np.polyfit(X_c[:,0], X_c[:, 1],1)\n",
    "reg_line,  = ax.plot(x_axis_full, m*x_axis_full + b, linestyle = 'dashed', color = 'black', alpha = 0.5)\n",
    "line,      = ax.plot(x_axis, m*x_axis + b, alpha = 1)\n",
    "ax.legend(['True Regression Line', 'Linear Fit'])\n",
    "the_vlines = []\n",
    "# vlines = ax.vlines(X_c[:,0], m*X_c[:,0] + b, X_c[:,1], linestyles='solid', color = 'black')\n",
    "\n",
    "sum_of_vert_dists = make_vlines(X_c[:,0], X_c[:, 1], m, b)\n",
    "\n",
    "# Display the sum of residuals\n",
    "resid_label = 'Sum of Vertical Distances (Residuals): '\n",
    "plt.text(-3.5,28,resid_label, weight='bold')\n",
    "resid_pt = plt.text(-.5, 28, '{:.2f}'.format(sum_of_vert_dists))\n",
    "\n",
    "def update_slope(val):\n",
    "    line.set_ydata(slope_slider.val*x_axis + b)\n",
    "    sum_of_vert_dists = make_vlines(X_c[:,0], X_c[:,1], slope_slider.val, b, first_run = False)\n",
    "    resid_pt.set_text('{:.2f}'.format(sum_of_vert_dists))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slope_slider.on_changed(update_slope)\n",
    "# ani = animation.FuncAnimation(\n",
    "#     fig, animate, interval=20, frames = 10, blit=False, save_count=50, repeat = True)\n",
    "\n",
    "# Create a `matplotlib.widgets.Button` to reset the sliders to initial values.\n",
    "resetax = plt.axes([0.8, 0.025, 0.1, 0.04])\n",
    "button = Button(resetax, 'Reset', hovercolor='0.975')\n",
    "\n",
    "def reset(event):\n",
    "    slope_slider.reset()\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "button.on_clicked(reset)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.arange(0, 2*np.pi, 0.01)\n",
    "line, = ax.plot(x, np.sin(x))\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    line.set_ydata(np.sin(x + i / 50))  # update the data.\n",
    "    return line,\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, animate, interval=20, blit=True, save_count=50)\n",
    "\n",
    "# HTML(ani.to_html5_video())\n",
    "# HTML(ani.to_jshtml())\n",
    "# To save the animation, use e.g.\n",
    "#\n",
    "# ani.save(\"movie.mp4\")\n",
    "#\n",
    "# or\n",
    "#\n",
    "# writer = animation.FFMpegWriter(\n",
    "#     fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "# ani.save(\"movie.mp4\", writer=writer)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbf0f0",
   "metadata": {},
   "source": [
    "# When Should We Use Each Method? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e612a4af",
   "metadata": {},
   "source": [
    "This is a great question. The context ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ba250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
