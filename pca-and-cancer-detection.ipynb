{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e6cdcf",
   "metadata": {},
   "source": [
    "<h1> PCA with the SVD and Eigendecomposition</h1>\n",
    "\n",
    "This is meant as a casual way to interact with some of the linear algebra concepts introduced in the course **Linear Algebra** in the [ICME Summer Workshops in Data Science](https://icme.stanford.edu/icme-summer-workshops-2021-fundamentals-data-science#LinAlg) (2021) series, taught by Professor Margot Gerritsen and Laura Lyman. \n",
    "\n",
    "If you have not interacted with a Jupyter notebook before, to execute a single code block (called a *cell*), press `Shift` followed by `Enter` on your machine. Keep in mind that some cells depend on variables defined in previous cells, so the code blocks should be executed in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e972fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   numpy import genfromtxt\n",
    "import numpy as np\n",
    "import math\n",
    "# Optional; suppresses scientific notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a917696",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Let's start off the morning with showing how even an *introductory* understanding of principal component analysis (pca) can help us understand some important real world examples. Say we are researching breast cancer. We are trying to understand the key characteristics of what makes a tumor cell benign (harmless) versus malignant (potentially dangerous). \n",
    "\n",
    "The following data from University of Wisconsin is taken from digitized images of fine needle aspirates (FNA) of a breast mass for each patient. The features describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "Source: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "\n",
    "In a study, there are many traits of a tumor that we can observe and measure. The following matrix is comprised of the data for the first 50 patients, who have a mix of benign and malignant tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c479b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = genfromtxt('WisconsinDataFirst50Cases.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd7a11",
   "metadata": {},
   "source": [
    "For simplicity, suppose we only consider the 9 following properties in the dataset.\n",
    "\n",
    "1. `radius_mean`\n",
    "2. `texture_mean` (standard deviation of the grayscale values in the 2D image)\n",
    "3. `perimeter_mean`\n",
    "4.  `area_mean`\t\n",
    "5. `smoothness_mean`\n",
    "6. `compactness_mean`\n",
    "7. `concavity_mean`\n",
    "8. `concave points_mean`\n",
    "9. `symmetry_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30186e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes header row and extra columns (features)\n",
    "X = dataset[1:,1:10]\n",
    "[m,n]     = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee5e20",
   "metadata": {},
   "source": [
    "Recall that the procedure for pca is as follows.\n",
    "\n",
    "1. Populate the _m_ x _n_ data matrix matrix _X_, where _m_ is the number of measurement types and _n_ is the number of explanatory variables \n",
    "2. Subtract off the mean for each measurement type (in our case, each column of the data matrix _X_)\n",
    "3. Calculate the principal components and PC scores by\n",
    " * Performing an SVD on _X_ (i.e. $X = U \\Sigma V^T$), or\n",
    " * Performing an eigendecomposition on the covariance matrix $C = \\frac{1}{n-1} X^T X$\n",
    " \n",
    "To accomplish step (2), we can use the following straight-forward code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367500a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_matrix(X):\n",
    "    [m,n]     = X.shape\n",
    "    # Subtract off the mean of each feature i.e. subtract from each column the avg. of that column \n",
    "    for feature_idx in range(n):\n",
    "        X[:,feature_idx] = X[:,feature_idx] - np.mean(X[:,feature_idx])\n",
    "    return(X)\n",
    "\n",
    "# Subtract off the mean of each feature i.e. subtract from each column the avg. of that column \n",
    "for feature_idx in range(n):\n",
    "    X[:,feature_idx] = X[:,feature_idx] - np.mean(X[:,feature_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9c2b9",
   "metadata": {},
   "source": [
    "Now we consider the first option within step (3), which involves using the SVD to factor our data matrix $X$ directly.\n",
    "\n",
    "## 2. Using the SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1462c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[U,Sigma,VT] = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1d0ca",
   "metadata": {},
   "source": [
    "The PC scores are the nonzero entries of $\\Sigma$, ranked from largest to smallest. The larger the PC score, the more its corresponding principal component (its eigenvector in $V$) characterizes the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "531a4ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2066.52718752   27.79295968   23.17301195    1.37560122    0.24598001\n",
      "    0.13129855    0.10029456    0.06102043    0.03562519]\n"
     ]
    }
   ],
   "source": [
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1ed38",
   "metadata": {},
   "source": [
    "The top two PC scores ($\\approx 2066.527$ and $\\approx 27.793$). Their eigenvectors are the first two columns of $V$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1489e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01012525 -0.05372279]\n",
      " [-0.00220572 -0.82526377]\n",
      " [-0.06819562 -0.5606423 ]\n",
      " [-0.99761814  0.04069534]\n",
      " [ 0.00001172 -0.00053788]\n",
      " [-0.00002646 -0.00605095]\n",
      " [-0.00009217 -0.00574415]\n",
      " [-0.0000641  -0.00182303]\n",
      " [ 0.00000156 -0.0021105 ]]\n",
      "0.9976181424527256\n",
      "0.8252637683444004\n"
     ]
    }
   ],
   "source": [
    "V = np.transpose(VT)\n",
    "# Show first two principal components of our data\n",
    "print(V[:,:2])\n",
    "print(max(abs(V[:,0])))\n",
    "print(max(abs(V[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f559a",
   "metadata": {},
   "source": [
    "## 3. Using the Eigendecomposition\n",
    "\n",
    "Suppose instead we use the eigendecomposition on the so-called *covariance matrix* $C$.  That is,\n",
    "\n",
    "$$C := \\frac{1}{n-1} X^T X = V \\Lambda V^T  $$\n",
    "\n",
    "Note that the $\\frac{1}{n-1}$ factor is absorbed into $\\Lambda$. The PC scores $\\sigma_i$ of $X$ are related to the eigenvalues $\\lambda_i$ of $C$ by the relationship \n",
    "$$\\sigma_i = \\sqrt{\\lambda_i}$$ \n",
    "\n",
    "(As a technical aside, we know that the eigenvalues of $C$ are guaranteed to be non-negative, since $C$ will always be what we call a *symmetric positive semi-definite matrix*). We can compute the top two PC scores now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a3833db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4270534.61676703,     772.44860773,     536.98848276,\n",
       "             1.8922787 ,       0.06050616,       0.01723931,\n",
       "             0.010059  ,       0.00372349,       0.00126915])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C               = np.transpose(X).dot(X)\n",
    "Lambda, V_eig   = np.linalg.eig(C)\n",
    "# Be careful! While the diagonal entries of Lambda are in descending \n",
    "# order (per the convention) in this example, in general `np.linalg.eig`\n",
    "# does *not* guarantee that the Lambda_ii are ordered \n",
    "Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cd1d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2066.5271875218655\n",
      "27.792959679276983\n"
     ]
    }
   ],
   "source": [
    "# From looking at Lambda, we see that the top two PC scores are its first two entries\n",
    "sigma_1_eig = math.sqrt(Lambda[0])\n",
    "sigma_2_eig = math.sqrt(Lambda[1])\n",
    "print(sigma_1_eig)\n",
    "print(sigma_2_eig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78439275",
   "metadata": {},
   "source": [
    "These are the same singular values (PC scores) $\\sigma_1, \\sigma_2$ found in the previous section by the SVD!\n",
    "\n",
    "The top two principal components are then the first two vectors of `V_eig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31bcc939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01012525  0.05372279]\n",
      " [-0.00220572  0.82526377]\n",
      " [-0.06819562  0.5606423 ]\n",
      " [-0.99761814 -0.04069534]\n",
      " [ 0.00001172  0.00053788]\n",
      " [-0.00002646  0.00605095]\n",
      " [-0.00009217  0.00574415]\n",
      " [-0.0000641   0.00182303]\n",
      " [ 0.00000156  0.0021105 ]]\n"
     ]
    }
   ],
   "source": [
    "# Show first two principal components of our data\n",
    "print(V_eig[:,:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd49ecf",
   "metadata": {},
   "source": [
    "This is the same result as before, at least for the first two principal components!\n",
    "\n",
    "Notice however that Python returned the second column of `V_eig` to be the 2nd column of `V_SVD` scaled by $-1$. This is okay; remember that eigenvectors are only concerned with *direction* and not scaling factors; those are handled by the PC scores $\\sigma_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3bcd46",
   "metadata": {},
   "source": [
    "## 4. Interpretation \n",
    " * For the first principal component, we can see that it is dominated by its 4th entry with magnitude `0.99761814` (where we note that the maximum magnitude of any entry in $V$ can be 1, since $V$ is an orthogonal matrix). This means that the first principal component  is best described by the 4th feature, which is `area_mean` i.e. the average 2D area of the breast mass in the digitized images.\n",
    "\n",
    "* For the second principal component, observe that it is biased toward the second and third variables (entries `-0.82526377` and `-0.560642`3, resp.), with all remaining values at least an order of magnitude smaller. This means the second principal component  &mdash; or our new characteristic describing these masses  &mdash; is a linear combination of `texture_mean` and `perimeter_mean`. In terms of the real world, what tumor characteristic describes a combination of `texture_mean` and `perimeter_mean`?\n",
    "\n",
    "* The variable `texture_mean` is the standard deviation of the grayscale values in the image, since topogrophy (or texture) is indicated by darker or ligher shading. Since this involves standard deviation, it means that a breast mass with a large `texture_mean` isn't necessarily large or tall; rather, its texture is non-uniform. Heuristically, we can imagine this as \"spikey-ness\", or an ill-defined and inconsistent border in its 3rd (vertical) dimension.\n",
    "\n",
    "* What about the variable `perimeter_mean`? Of course, a tumor with a larger area will necessarily have a larger perimeter. However, for the characteristic the second principal component represents, we know that `perimeter_mean` indicates information that is *somewhat independent* of the tumor's area; otherwise, the fourth entry of this eigenvector (which corresponds to `area_mean`) would be larger in magnitude. So how can we vary the size of a tumor's perimeter while *minimally* affecting its area? \n",
    "\n",
    "* One way to increase the perimeter without increasing the area is to give the mass tall and skinny spikes protruding from its border; the perimeter is increased by traveling up and down the spikes, while minimal area is gained since the spikes are so thin. In this sense, `perimeter_mean` on its own can be a metric for how much zigzagging occurs on the breast mass's border.\n",
    "\n",
    "* Therefore, the second principal component could be described as \"overall spikey-ness or fuziness of the tumor's border.\"\n",
    "\n",
    "**Principal component analysis tells us these characteristics directly**. Again, this is real data, and the interpretation here is not biased by prior medical knowledge. However, if we then Google defining characteristics for determining benign or malignant breast tumors, we can see right away that two important hallmarks are a *tumor's size* (i.e. its area) and whether it has an *irregular shape with spiky or fuzzy edges*.\n",
    "\n",
    "Note that the top PC score ($\\approx 2066.527$) is an order of magnitude larger than all the other singular values. This means that, while \"overall spikey-ness or fuziness\" the second-most important attribute, ultimately the first principal component (average tumor area) has the most influence over characterizing the dataset as a whole.\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
